{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the Bitcoin price data 'Open', 'High', 'Low', 'Close', and 'Volume'\n",
    "btc_data = pd.read_csv('btc.csv')\n",
    "# Drop the date column\n",
    "data = np.stack([btc_data['Close'].values, btc_data['Open'].values, btc_data['High'].values, btc_data['Low'].values], axis=1)\n",
    "# Normalize the data to be between 0 and 1\n",
    "data_norm = (data - data.mean()) / (data.std())\n",
    "train_size = int(len(data_norm) * 0.8)\n",
    "test_size = len(data_norm) - train_size\n",
    "# Split the data into training and testing sets\n",
    "train_data = data_norm[:train_size]\n",
    "test_data = data_norm[test_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(582836, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.expand_dims(train_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 466268, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 14:56:31.469066: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7286/7286 [==============================] - 4s 462us/step - loss: 0.3584\n",
      "Epoch 2/50\n",
      "7286/7286 [==============================] - 3s 415us/step - loss: 0.3487\n",
      "Epoch 3/50\n",
      "7286/7286 [==============================] - 3s 413us/step - loss: 0.3487\n",
      "Epoch 4/50\n",
      "7286/7286 [==============================] - 3s 415us/step - loss: 0.3487\n",
      "Epoch 5/50\n",
      "7286/7286 [==============================] - 3s 413us/step - loss: 0.3487\n",
      "Epoch 6/50\n",
      "7286/7286 [==============================] - 3s 430us/step - loss: 0.3487\n",
      "Epoch 7/50\n",
      "7286/7286 [==============================] - 3s 440us/step - loss: 0.3487\n",
      "Epoch 8/50\n",
      "7286/7286 [==============================] - 3s 427us/step - loss: 0.3487\n",
      "Epoch 9/50\n",
      "7286/7286 [==============================] - 3s 428us/step - loss: 0.3487\n",
      "Epoch 10/50\n",
      "7286/7286 [==============================] - 3s 434us/step - loss: 0.3487\n",
      "Epoch 11/50\n",
      "7286/7286 [==============================] - 3s 450us/step - loss: 0.3487\n",
      "Epoch 12/50\n",
      "7286/7286 [==============================] - 3s 434us/step - loss: 0.3487\n",
      "Epoch 13/50\n",
      "7286/7286 [==============================] - 3s 430us/step - loss: 0.3487\n",
      "Epoch 14/50\n",
      "7286/7286 [==============================] - 3s 442us/step - loss: 0.3487\n",
      "Epoch 15/50\n",
      "7286/7286 [==============================] - 3s 415us/step - loss: 0.3487\n",
      "Epoch 16/50\n",
      "7286/7286 [==============================] - 3s 423us/step - loss: 0.3487\n",
      "Epoch 17/50\n",
      "7286/7286 [==============================] - 3s 420us/step - loss: 0.3487\n",
      "Epoch 18/50\n",
      "7286/7286 [==============================] - 3s 423us/step - loss: 0.3487\n",
      "Epoch 19/50\n",
      "7286/7286 [==============================] - 3s 442us/step - loss: 0.3487\n",
      "Epoch 20/50\n",
      "7286/7286 [==============================] - 3s 447us/step - loss: 0.3487\n",
      "Epoch 21/50\n",
      "7286/7286 [==============================] - 3s 428us/step - loss: 0.3487\n",
      "Epoch 22/50\n",
      "7286/7286 [==============================] - 3s 416us/step - loss: 0.3487\n",
      "Epoch 23/50\n",
      "7286/7286 [==============================] - 3s 440us/step - loss: 0.3487\n",
      "Epoch 24/50\n",
      "7286/7286 [==============================] - 3s 445us/step - loss: 0.3487\n",
      "Epoch 25/50\n",
      "7286/7286 [==============================] - 3s 424us/step - loss: 0.3487\n",
      "Epoch 26/50\n",
      "7286/7286 [==============================] - 3s 419us/step - loss: 0.3487\n",
      "Epoch 27/50\n",
      "7286/7286 [==============================] - 3s 420us/step - loss: 0.3487\n",
      "Epoch 28/50\n",
      "7286/7286 [==============================] - 3s 416us/step - loss: 0.3487\n",
      "Epoch 29/50\n",
      "7286/7286 [==============================] - 3s 409us/step - loss: 0.3487\n",
      "Epoch 30/50\n",
      "7286/7286 [==============================] - 3s 406us/step - loss: 0.3487\n",
      "Epoch 31/50\n",
      "7286/7286 [==============================] - 3s 416us/step - loss: 0.3487\n",
      "Epoch 32/50\n",
      "7286/7286 [==============================] - 3s 426us/step - loss: 0.3487\n",
      "Epoch 33/50\n",
      "7286/7286 [==============================] - 3s 417us/step - loss: 0.3487\n",
      "Epoch 34/50\n",
      "7286/7286 [==============================] - 3s 431us/step - loss: 0.3487\n",
      "Epoch 35/50\n",
      "7286/7286 [==============================] - 3s 416us/step - loss: 0.3487\n",
      "Epoch 36/50\n",
      "7286/7286 [==============================] - 3s 404us/step - loss: 0.3487\n",
      "Epoch 37/50\n",
      "7286/7286 [==============================] - 3s 402us/step - loss: 0.3487\n",
      "Epoch 38/50\n",
      "7286/7286 [==============================] - 3s 403us/step - loss: 0.3487\n",
      "Epoch 39/50\n",
      "7286/7286 [==============================] - 3s 403us/step - loss: 0.3487\n",
      "Epoch 40/50\n",
      "7286/7286 [==============================] - 3s 402us/step - loss: 0.3487\n",
      "Epoch 41/50\n",
      "7286/7286 [==============================] - 3s 402us/step - loss: 0.3487\n",
      "Epoch 42/50\n",
      "7286/7286 [==============================] - 3s 440us/step - loss: 0.3487\n",
      "Epoch 43/50\n",
      "7286/7286 [==============================] - 3s 442us/step - loss: 0.3487\n",
      "Epoch 44/50\n",
      "7286/7286 [==============================] - 3s 428us/step - loss: 0.3487\n",
      "Epoch 45/50\n",
      "7286/7286 [==============================] - 3s 445us/step - loss: 0.3487\n",
      "Epoch 46/50\n",
      "7286/7286 [==============================] - 3s 419us/step - loss: 0.3487\n",
      "Epoch 47/50\n",
      "7286/7286 [==============================] - 3s 411us/step - loss: 0.3487\n",
      "Epoch 48/50\n",
      "7286/7286 [==============================] - 3s 417us/step - loss: 0.3487\n",
      "Epoch 49/50\n",
      "7286/7286 [==============================] - 3s 419us/step - loss: 0.3487\n",
      "Epoch 50/50\n",
      "7286/7286 [==============================] - 3s 418us/step - loss: 0.3487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17d1dd120>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the Bitcoin price data 'Open', 'High', 'Low', 'Close', and 'Volume'\n",
    "btc_data = pd.read_csv('btc.csv')\n",
    "# Drop the date column\n",
    "data = np.stack([btc_data['Close'].values, btc_data['Open'].values, btc_data['High'].values, btc_data['Low'].values], axis=1)\n",
    "# Normalize the data to be between 0 and 1\n",
    "data_norm = (data - data.mean()) / (data.std())\n",
    "train_size = int(len(data_norm) * 0.8)\n",
    "test_size = len(data_norm) - train_size\n",
    "# Split the data into training and testing sets\n",
    "train_data = data_norm[:train_size]\n",
    "test_data = data_norm[test_size:]\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "# Define the model\n",
    "input_dim = train_data.shape[1]\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(Dense(16, input_shape=(input_dim,), activation='relu'))\n",
    "autoencoder.add(Dense(8, activation='relu'))\n",
    "autoencoder.add(Dense(16, activation='relu'))\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(train_data, train_data, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('autoencoder_model.h5') # save the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this new transformer autoencoder? \n",
    "    1. Dimensionality reduction: You can use the encoder part of the autoencoder to reduce the dimensionality of your data, allowing you to represent high-dimensional data in a lower-dimensional space, which can be easier to visualize, analyze, and model.\n",
    "\n",
    "    2. Data denoising: You can use the autoencoder to denoise data by training it on noisy data and then using the decoder part of the autoencoder to reconstruct the original (clean) data from the noisy data.\n",
    "\n",
    "    3. Data compression: You can use the autoencoder to compress data by training it on a large dataset and then using the encoder part of the autoencoder to compress new data. This can be useful when dealing with large datasets that require a lot of storage space.\n",
    "\n",
    "    4. Anomaly detection: You can use the autoencoder to detect anomalies in data by training it on a normal dataset and then using it to reconstruct new data. Data that cannot be well-reconstructed by the autoencoder is likely to be anomalous.\n",
    "\n",
    "These are just a few examples of what you can do with an autoencoder. The specific application of the autoencoder will depend on the nature of your data and the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m last_price \u001b[39m=\u001b[39m data[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m encoded_last_price \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39;49mencoder(last_price)\n\u001b[1;32m     21\u001b[0m predicted_next_price \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39minverse_transform(autoencoder\u001b[39m.\u001b[39mdecoder(encoded_last_price))[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicted next Bitcoin price: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_next_price\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "# Predict bitcoin prices\n",
    "\n",
    "# Load the trained autoencoder\n",
    "autoencoder = load_model('autoencoder_model.h5')\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Reshape the data for input to the autoencoder\n",
    "data = data.reshape((len(data), -1))\n",
    "\n",
    "# Encode the data using the trained autoencoder\n",
    "encoded_data = autoencoder.encoder(data)\n",
    "\n",
    "# Predict the next Bitcoin price using the encoded data\n",
    "import numpy as np\n",
    "last_price = data[-1].reshape((1, -1))\n",
    "encoded_last_price = autoencoder.encoder(last_price)\n",
    "predicted_next_price = scaler.inverse_transform(autoencoder.decoder(encoded_last_price))[0][0]\n",
    "\n",
    "print(f\"Predicted next Bitcoin price: {predicted_next_price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 4])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backtesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd7ef08011097067a9ce6d3088f8c54f82b5a0a2b35d2fd25745095997af54cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
